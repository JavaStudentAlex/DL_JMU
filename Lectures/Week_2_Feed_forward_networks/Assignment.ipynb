{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1244ba15faa46633",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "Supervisor: Gregor Geigle\n",
    "If you have questions, errors or problems:\n",
    "1) Open a thread in the WueCampus forum: https://wuecampus.uni-wuerzburg.de/moodle/mod/forum/view.php?id=3093380\n",
    "2) If your question/problem is personal and not appropriate for the forum, send a mail to gregor.geigle@uni-wuerzburg.de."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f517af10a6fcdc5c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 0 Setting Up\n",
    "\n",
    "In this exercise, you will write your own deep learning model code. Before you can run the code here, you will have to install the dependencies.\n",
    "You are given code skeletons and you will have to complete them according to the task. DO NOT CHANGE THE CODE SKELETONS.\n",
    "\n",
    "The exercise requires:\n",
    "0) Python Version 3.9 or greater (obviously)\n",
    "1) [torch (PyTorch)](https://pytorch.org/get-started/locally/) -> PyTorch is a library for working with tensors on GPUs (or CPUs; you do not need a GPU for the exercises). This is **the** library for implementing and training deep learning models and is the main library for the exercises.\n",
    "2) [Numpy](https://numpy.org/install/) -> Another library for working with tensors. It is CPU-only.\n",
    "3) [scipy](https://scipy.org/install/) -> A library build on numpy for various scientific computing. You will not use this library yourself but some functions are needed for the code.\n",
    "\n",
    "To install the requirements, you can run `pip install torch numpy scipy`. Once done, you can run the cell below and if it executes without error, you are ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf65ec6c87ad5c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import scipy\n",
    "\n",
    "torch.set_printoptions(precision=2) # limit precision when printing tensors to 2 digits\n",
    "# This cell should run without problem once you install all requirements. If you get an ImportError, you forgot to install one of the libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50871a0240302360",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1 Layers and Activations and Initializations\n",
    "\n",
    "In the first part of this exercise, you will implement different layers and activation functions in PyTorch yourself by completing the provided code skeletons.\n",
    "\n",
    "IMPORTANT: PyTorch already implements all the layers and activation functions. You are (obviously) not allowed to use these functions. You are limited to basic operations (like `torch.matmul (Matrix multiplication)/ @ (the operation symbol for matrix multiplication), * (element-wise multiplication), torch.exp, torch.log, torch.sum)`. \n",
    "\n",
    "You can read more about those functions in the [official documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572fd649c32fd8ba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.1 Matrix multiplication warmup\n",
    "\n",
    "This exercise is to be done by hand. You are given three matrices `A=[[a,b,c],[d,e,f]], B=[[g,h],[i,j],[k,l]], C=[[m,n],[o,p]]`.\n",
    "Compute the following or indicate if this operation is not allowed. \n",
    "1. A @ B [@ is the matrix multiplication]\n",
    "2. B @ A\n",
    "3. A @ C\n",
    "4. B @ C\n",
    "5. A * B.T  [* is the element-wise multiplication, .T indicates a transposed matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df73c4163f1a59",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Fill in your answer and do not otherwise modify this cell**\n",
    "\n",
    "1. \n",
    "$$\n",
    "\\begin{align*}\n",
    "A @ B &= \n",
    "\\begin{bmatrix}\n",
    "a * g + b * i + c * k & a * h + b * j + c * l \\\\\n",
    "d * g + e * i + f * k & d * h + e * j + f * l\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "\n",
    "2. \n",
    "$$\n",
    "\\begin{align*}\n",
    "B @ A &= \n",
    "\\begin{bmatrix}\n",
    "g * a + h * d & g * b + h * e & g * c + h * f \\\\\n",
    "i * a + j * d & i * b + j * e & i * c + j * f \\\\\n",
    "k * a + l * d & k * b + l * e & k * c + l * f\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "3. \n",
    "$$\n",
    "\\begin{align*}\n",
    "B @ C &= \n",
    "\\begin{bmatrix}\n",
    "g * m + h * o & g * n + h * p \\\\\n",
    "i * m + j * o & i * n + j * p \\\\\n",
    "k * m + l * o & k * n + l * p\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "4.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "A * B^T &= \n",
    "\\begin{bmatrix}\n",
    "a * g & b * i & c * k \\\\\n",
    "d * h & e * j & f * l\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26cc8b548fc19af",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.2 Building up towards complex blocks\n",
    "\n",
    "In this exercise, we will start with implementing fundamental layers and then combine those layers into more complex modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea9293c864204bd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \"\"\"\n",
    "    This class is the base class for all layers and modules that you will implement.\n",
    "    \"\"\"        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return the result of the forward pass of this layer. Save intermediate results\n",
    "        necessary to compute the gradients in the backward pass. \n",
    "        \"\"\"\n",
    "        raise NotImplementedError('Base class - method is not implemented')\n",
    "    \n",
    "def test_module(module, input=None):\n",
    "    if input is None:\n",
    "        input = torch.randn((2, 3))\n",
    "    print(f\"Input: {input}\")\n",
    "    result = module.forward(input)\n",
    "    print(f\"Output: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42640f92ad9e14c9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.2.1 Activation Functions\n",
    "As you have seen in the lecture, there are many different activation functions. For this exercise, we will implement the ReLU activation from the lecture and also one function called **SiLU** or Sigmoid Linear Unit. We will first implement the Sigmoid and then use this for the SiLU module.\n",
    "\n",
    "The definition for the functions is:\n",
    "$$ \\mathrm{ReLU}(x) = \\max(0, x) $$\n",
    "$$ \\mathrm{Sigmoid}(x) = \\frac{1}{1+\\exp(-x)} $$\n",
    "$$ \\mathrm{SiLU}(x) = x * \\mathrm{Sigmoid}(x) $$\n",
    "\n",
    "(Hint for ReLU: `torch.clamp`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76a9adcee3a68442",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return the element-wise ReLU of the input.\n",
    "            param: x (torch.tensor): input to the activation function, of arbitrary shape\n",
    "            returns (torch.tensor): element-wise ReLU(x), of the same shape as x\n",
    "        \"\"\"\n",
    "        return torch.clamp(x, min=0)\n",
    "\n",
    "class Sigmoid(Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return the element-wise sigmoid of the input.\n",
    "            param: x (torch.tensor): input to the activation function, of arbitrary shape\n",
    "            returns (torch.tensor): element-wise sigmoid(x), of the same shape as x\n",
    "        \"\"\"\n",
    "        return 1/(1+torch.exp(-x))\n",
    "        \n",
    "        \n",
    "class SiLU(Module):\n",
    "    def __init__(self):\n",
    "        self.sigmoid = Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Return the result of a SiLU activation of the input.\n",
    "            param: x (torch.tensor): input to the activation function, of arbitrary shape\n",
    "            returns (torch.tensor): element-wise SiLU(x), of the same shape as x\n",
    "        \"\"\"\n",
    "        return self.sigmoid.forward(x) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aca7cd9b19529835",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([[ 1.71,  1.02, -1.40],\n",
      "        [-0.13, -0.90,  0.85]])\n",
      "Output: tensor([[1.71, 1.02, 0.00],\n",
      "        [0.00, 0.00, 0.85]])\n",
      "\n",
      "Input: tensor([[ 1.50, -0.32,  0.09],\n",
      "        [ 0.92, -0.37,  0.04]])\n",
      "Output: tensor([[0.82, 0.42, 0.52],\n",
      "        [0.72, 0.41, 0.51]])\n",
      "\n",
      "Input: tensor([-100,   -1,    0,    1,  100])\n",
      "Output: tensor([0.00, 0.27, 0.50, 0.73, 1.00])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can manually test your implementation using the test_module function. Feel free to change this cell and re-run it for your tests.\n",
    "test_module(ReLU()) # creates a random tensor as input\n",
    "test_module(Sigmoid()) # creates a random tensor as input\n",
    "test_module(Sigmoid(), input=torch.tensor([-100, -1, 0, 1, 100])) # test with a specific tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1539b5f784d69ab3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f391ff32ef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatic tests to verify your solution. DO NOT CHANGE THIS CELL\n",
    "%run Tests/TestActivationFunctions.py\n",
    "TestReLU.ReLU = ReLU\n",
    "TestSigmoid.Sigmoid = Sigmoid\n",
    "TestSiLU.SiLU = SiLU\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b902cc2e7d454",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.2.2 Fully Connected Layer\n",
    "Next, we are implementing the fully connected layer. This is your first parametrized module, that is, the first module that has parameters that would be optimized during training of a model.\n",
    "\n",
    "Given a weight matrix $W \\in \\mathbb{R}^{n \\times m}$ and a bias vector $b \\in \\mathbb{R}^{m}$, the output of the fully connected layer is $xW + b$.\n",
    "\n",
    "It is important to properly *initialize* the parameters. For this exercise we will keep it simple and initialize $W$ with values from a normal distribution with mean 0 and standard deviation 0.02, and $b$ is initialized with all 0s. Hint: `torch.normal, torch.zeros`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "136388ea55a840e0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \"\"\" A fully connected layer.\n",
    "            param: input_size (int): dimension n of the input vector\n",
    "            param: output_size (int): dimension m of the output vector\n",
    "        \"\"\"\n",
    "        self.weights = self.init_weights(input_size, output_size, is_bias=False)\n",
    "        self.bias = self.init_weights(output_size, is_bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Compute the foward pass through the layer.\n",
    "            param: x (torch.tensor): input with shape [b, n] where b is the batch size and n is the input size\n",
    "            returns (torch.tensor): result of the forward pass, of shape [b, m] where b is the batch size and\n",
    "                   m is the output size\n",
    "        \"\"\"\n",
    "        return x @ self.weights + self.bias\n",
    "    \n",
    "    def init_weights(self, *dimensions, is_bias=False):\n",
    "        \"\"\" Create a tensor of weights. The bias is initialized with all zeros. The weights are initialized with values from a normal distribution with mean 0 and standard deviation 0.02\n",
    "            param: dimensions (list of ints): the dimensions of the tensor\n",
    "            returns (torch.tensor): the tensor of weights\n",
    "        \"\"\"\n",
    "        return torch.zeros(dimensions) if is_bias else torch.normal(0, 0.02, dimensions)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d6170cfbd83477",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n",
      "tensor([[-0.01,  0.00,  0.02],\n",
      "        [-0.00, -0.01, -0.04],\n",
      "        [ 0.00, -0.00, -0.00],\n",
      "        [ 0.03, -0.07, -0.03]])\n",
      "Input: tensor([[-1.82, -1.64,  0.20, -1.07],\n",
      "        [-0.36,  0.12, -0.42, -0.44]])\n",
      "Output: tensor([[-0.01,  0.08,  0.05],\n",
      "        [-0.01,  0.03,  0.00]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feel free to change this cell and re-run it for your tests.\n",
    "fc = FullyConnectedLayer(4, 3)\n",
    "print(fc.bias)\n",
    "print(fc.weights)\n",
    "test_module(fc, input=torch.randn((2, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7e4712ff17abfca",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "........\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 0.080s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f391fe268f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatic tests to verify your solution. DO NOT CHANGE THIS CELL\n",
    "%run Tests/TestFullyConnected.py\n",
    "TestFullyConnected.FullyConnected = FullyConnectedLayer\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823407ee2b3d177",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.2.3 Putting it all together\n",
    "\n",
    "More complex deep learning architectures often contain *blocks* made of different layers that are repeated multiple times.\n",
    "To reduce code repetition, we create modules that encapsulate those blocks.\n",
    "\n",
    "Here, we will create a block for a 2-layer fully connected network with an activation function (SiLU) inbetween.\n",
    "Do not implement this from scratch but use your above implementations for SiLU and fully connected layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "601863ab9ec504fb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SiLUBlock(Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\" A fully connected layer.\n",
    "            param: input_size (int): dimension n of the input vector\n",
    "            param: hidden_size (int): dimension h of the hidden vector\n",
    "            param: output_size (int): dimension m of the output vector\n",
    "        \"\"\"\n",
    "        # TODO: define the necessary class variables\n",
    "        self.fully_connected_in = FullyConnectedLayer(input_size, hidden_size)\n",
    "        self.fully_connected_out = FullyConnectedLayer(hidden_size, output_size)\n",
    "        self.activation = SiLU()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Compute the foward pass through the layer.\n",
    "            param: x (torch.tensor): input with shape [b, n] where b is the batch size and n is the input size\n",
    "            returns (torch.tensor): result of the forward pass, of shape [b, m] where b is the batch size and\n",
    "                   m is the output size\n",
    "        \"\"\"\n",
    "        return self.fully_connected_out.forward(self.activation.forward(self.fully_connected_in.forward(x)))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffa704f2fdf54552",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..........\n",
      "----------------------------------------------------------------------\n",
      "Ran 10 tests in 0.073s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f391fe31a50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatic tests to verify your solution. DO NOT CHANGE THIS CELL\n",
    "%run Tests/TestBlocks.py\n",
    "TestSiLUBlock.SiLUBlock = SiLUBlock\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1319099cd9827ed",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.2.4 BONUS: The SiGLU Blocks (not graded)\n",
    "\n",
    "This is an ungraded bonus exercise that combines all previously learned in one block. \n",
    "The SiGLU (or SwiGLU; for SiLU or Swish Gated Linear Unit) block that you implement in this exercise is a very popular block in the real world as it is used in the majority of large language models.\n",
    "\n",
    "Its implementation is similar to the SiLU block before but we add an additional gating part:\n",
    "\n",
    "Given an input matrix $I \\in \\mathbb{R}^{n \\times h}$, a gate matrix $G \\in \\mathbb{R}^{n \\times h}$, and an output matrix $O \\in \\mathbb{R}^{h \\times m}$ (together with matching biases), the SiGLU block is defined as:\n",
    "$$ (xI * \\mathrm{SiLU}(xG))O $$ \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e607e47e1a99292",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SiGLUBlock(Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\" A fully connected layer.\n",
    "            param: input_size (int): dimension n of the input vector\n",
    "            param: hidden_size (int): dimension h of the hidden vector\n",
    "            param: output_size (int): dimension m of the output vector\n",
    "        \"\"\"\n",
    "        # TODO: define the necessary class variables\n",
    "        self.fully_connected_in = FullyConnectedLayer(input_size, hidden_size)\n",
    "        self.fully_connected_gate = FullyConnectedLayer(input_size, hidden_size)\n",
    "        self.fully_connected_out = FullyConnectedLayer(hidden_size, output_size)\n",
    "        self.activation = SiLU()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Compute the foward pass through the layer.\n",
    "            param: x (torch.tensor): input with shape [b, n] where b is the batch size and n is the input size\n",
    "            returns (torch.tensor): result of the forward pass, of shape [b, m] where b is the batch size and\n",
    "                   m is the output size\n",
    "        \"\"\"\n",
    "        return self.fully_connected_out.forward(\n",
    "            self.fully_connected_in.forward(x) * self.activation.forward(self.fully_connected_gate.forward(x))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b34127e421544bb6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "............\n",
      "----------------------------------------------------------------------\n",
      "Ran 12 tests in 0.077s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f3a0c316b30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatic tests to verify your solution. DO NOT CHANGE THIS CELL\n",
    "%run Tests/TestBlocks2.py\n",
    "TestSiGLUBlock.SiGLUBlock = SiGLUBlock\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeafb881e669e38",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In practice, we want the SiGLUBlock to use the same number of parameters as the SiLUBlock. Assuming we have input and output dimension $h$ and the SiLUBlock has hidden dimension $2h$, what is the hidden dimension of the SiGLUBlock with equivalent number of parameters?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5532f3169819b7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Fill in your answer and do not otherwise modify this cell**\n",
    " \n",
    "The number of parameters in SiLu block is $4h^2 + 3h$. At the same time the number of parameters in SwiGLU block is $3h x$, where $x$ is the hidden output size.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "3h x &= 4h^2 + 3h \\\\\n",
    "x &= \\frac{4}{3}h + 1\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5af38ef944ca9f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2 The Softmax Activation\n",
    "\n",
    "In the second part of the exercise, we will look at the softmax activation function in some more detail.\n",
    "\n",
    "The softmax function is a useful function because it can turn any arbitrary vector into a probability distribution (i.e., all terms are $\\geq 0$ and add up to 1).\n",
    "With the softmax, you can, for example, turn a model's output for a classification task with 3 labels into a distribution over those 3 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e06d159df51498c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1 Implementation of Softmax\n",
    "\n",
    "Implement the softmax module.\n",
    "Recall the definition $\\mathrm{softmax}(x) = \\frac{\\exp x}{\\sum_{x_i \\in x} \\exp x_i}$\n",
    "\n",
    "Your implementation has to be written **fully** with PyTorch functions, in other words, you are not allowed to use Python loops to sum up the values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77090ec82078f74c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Compute the forward pass through the layer.\n",
    "            param: x (torch.tensor): input with shape [b, n] where b is the batch size and n is the input size\n",
    "            returns (torch.tensor): result of the forward pass, of shape [b, n] where b is the batch size and n is the input size\n",
    "        \"\"\"\n",
    "        return torch.exp(x) / torch.sum(torch.exp(x), dim=1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "620e6d4bd56be63",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..............\n",
      "----------------------------------------------------------------------\n",
      "Ran 14 tests in 0.118s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f391ff33010>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatic tests to verify your solution. DO NOT CHANGE THIS CELL\n",
    "%run Tests/TestSoftmax.py\n",
    "TestSoftmax.Softmax = Softmax\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751470b4264c92b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2 More Stable Implementation\n",
    "\n",
    "The previous \"naive\" implementation of the softmax is not numerically stable because large values can cause overflow with the exponent (note the `nan` or \"Not a Number\" in the following output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8a41d3cd80e8192",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., nan]])\n"
     ]
    }
   ],
   "source": [
    "unstable_implementation = Softmax()\n",
    "tensor = torch.tensor([[0.1, 0.2, 1000]])\n",
    "\n",
    "print(unstable_implementation.forward(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c443618050bcee8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Implement a more stable version of softmax. Use the fact that $\\mathrm{softmax}(x)=\\mathrm{softmax}(x-b)$ with a good choice of $b$ to prevent overflow.\n",
    "\n",
    "Note: You are **strongly** encouraged to derive on paper why this equality is correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d359049d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "Softmax(x - b) &= \\frac{e^{x-b}}{\\sum_i^M e^{x-b}} = \\frac{e^{x} \\times e^{-b}}{e^{-b} \\times \\sum_i^M e^{x}} = \\frac{e^{x}}{\\sum_i^M e^{x}} = Softmax(x)\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14935fcf6d40fae2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StableSoftmax(Softmax):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\" Compute the forward pass through the layer.\n",
    "            param: x (torch.tensor): input with shape [b, n] where b is the batch size and n is the input size\n",
    "            returns (torch.tensor): result of the forward pass, of shape [b, n] where b is the batch size and n is the input size\n",
    "        \"\"\"\n",
    "        return torch.exp(x - torch.max(x, dim=1).values.view(-1, 1)) / torch.sum(torch.exp(x - torch.max(x, dim=1).values.view(-1, 1)), dim=1).view(-1, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e99276bb231d3187",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "stable_implementation = StableSoftmax()\n",
    "tensor = torch.tensor([[0.1, 0.2, 1000]])\n",
    "\n",
    "print(stable_implementation.forward(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7deb1484ff8971c1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".................\n",
      "----------------------------------------------------------------------\n",
      "Ran 17 tests in 0.312s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x7f3a0c4041f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatic tests to verify your solution. DO NOT CHANGE THIS CELL\n",
    "%run Tests/TestSoftmaxStable.py\n",
    "TestStableSoftmax.StableSoftmax = StableSoftmax\n",
    "unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
