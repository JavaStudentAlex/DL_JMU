{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKgSqPFiNAQ5"
   },
   "source": [
    "# Exercise # 2 \n",
    "\n",
    "Instructor for Assignment:\n",
    " - Saad Obaid ul Islam: saad.obaid-ul-islam@uni-wuerzburg.de\n",
    " - Kindly post your questions here: https://wuecampus.uni-wuerzburg.de/moodle/mod/forum/view.php?id=3093380 \n",
    "\n",
    "In this exercise, you will learn about Neural Network optimization using loss functions and backpropagation algorithm. You will also complete the code for your own miniature reverse mode automatic differentiation framework.   \n",
    "\n",
    "Note: Notation may not be the same as the lecture. In that case, kindly open an issue at the forum. DO NOT CHANGE THE CODE SKELETONS, JUST COMPLETE THE MISSING PARTS, as instructed.\n",
    "\n",
    "Pre-requistes:\n",
    "1. Python >= 3.8\n",
    "2. [torch (PyTorch)](https://pytorch.org/get-started/locally/) -> PyTorch is a library for working with tensors on GPUs (or CPUs; you do not need a GPU for the exericses). This is **the** library for implementing and training deep learning models.\n",
    "3. [Numpy](https://numpy.org/install/) -> Library for working with tensors. It is CPU-only.\n",
    "4. [maplotlib](https://pypi.org/project/matplotlib/) -> Library for creating visualizations. You will not be using this library but it is important to install it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IRtoR8LgSx0W"
   },
   "source": [
    "## Loss and Optimization\n",
    "\n",
    "A loss function, often called cost function, is a function that computes the difference between the ground truth and predicted output. In this exercise, you will learn about a simple loss function, known as Mean Squared Errors.\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2  \n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "\n",
    "1.   $n$ is the total number of data points.\n",
    "2.   $y_i$ is the ground truth.\n",
    "3.   $\\hat{y}_i$ is the predicted output computed as $x_iθ^T$ where $x_i$ is the input and $\\theta$ is the model parameter.\n",
    "\n",
    "To understand the $MSE$ loss function, imagine fitting a line through multiple data points, the $MSE$ will calculate how close the line is to each data point.\n",
    "\n",
    " ![image.png](data:image/png;base64,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)\n",
    "\n",
    "\n",
    " The above loss/cost function is used in optimization algorithms like Gradient Descent.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    " ## Gradient Descent\n",
    "\n",
    "Gradient Descent (GD) is an optimization algorithm that minimizes the objective function (cost/loss function) $J(θ)$.\n",
    "\n",
    "$\\text{For each parameter } \\theta, \\text{ update:} \\\\\n",
    "\\theta_{new} := \\theta_{old} - \\eta \\frac{\\partial}{\\partial \\theta} J(\\theta)$\n",
    "\n",
    "Where:\n",
    "1. $\\theta_{old}$ is parameter before the update.\n",
    "2. $\\eta$ is the learning rate (step size).\n",
    "3. $J(\\theta)$ is the cost function (in our case, $MSE$ loss).\n",
    "4. $\\theta_{new}$ is the updated parameter.\n",
    "\n",
    "Now, when we use $MSE$ loss in the gradient descent algorithm, the algorithm becomes.\n",
    "\n",
    "$\n",
    "\\theta_{new} := \\theta_{old} - \\eta \\frac{\\partial}{\\partial \\theta} \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$\n",
    "\n",
    "The partial derivative of MSE loss function is as follows considering $\\hat{y}_i = \\theta^T x_i$\n",
    "\n",
    "$\n",
    "    \\frac{\\partial L}{\\partial \\theta} = \\frac{2}{n} \\sum_{i=1}^{n} x_{ij} (y_i - \\hat{y}_i)\n",
    "$\n",
    "\n",
    "Using the gradient, we can update each parameter $\\theta_{new}$ in the model:\n",
    "\n",
    "$\n",
    "  \\theta_{new} = \\theta_{old} - \\eta \\cdot \\frac{2}{n} \\sum_{i=1}^{n} x_{ij} (y_i - \\hat{y}_i)\n",
    "$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "You will now write code for the $MSE$ loss function and **gradient descent** algorithm using python **numpy** module. Use the comments and hints provided.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lS2lTk3OHbEm"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hOJPOaxLI69X"
   },
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "\n",
    "    # predictions = input * theta\n",
    "\n",
    "    # cost = (1/n) * sum of ((predictions - y)^2)\n",
    "    return np.mean(np.square(np.dot(X, theta) - y))\n",
    "\n",
    "# Hint: Use the following\n",
    "# np.sum(): https://numpy.org/doc/stable/reference/generated/numpy.sum.html\n",
    "# np.square(): https://numpy.org/doc/stable/reference/generated/numpy.square.html\n",
    "\n",
    "# Gradient Descent\n",
    "def gradient_descent(X, y, theta, learning_rate, iterations):\n",
    "    n = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "\n",
    "    # for each iteration\n",
    "\n",
    "        # prediction = input . theta\n",
    "\n",
    "        # errors = predictions - y\n",
    "\n",
    "        # compute gradient of cost function: Δθ = (2*eta/n) * X.T * errors\n",
    "\n",
    "        # update theta_new: θ = θ - Δθ\n",
    "        # Store cost in history\n",
    "    for i in range(iterations):\n",
    "        predictions = np.dot(X, theta)\n",
    "        errors = np.subtract(predictions, y)\n",
    "        gradient = np.dot(X.T, errors) / n\n",
    "        theta -= learning_rate * gradient\n",
    "        cost_history[i] = compute_cost(X, y, theta)\n",
    "\n",
    "    return theta, cost_history\n",
    "\n",
    "# Hint:\n",
    "# Use np.dot: https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n",
    "# Use np.subtract: https://numpy.org/doc/stable/reference/generated/numpy.subtract.html\n",
    "# Use X.transpose(): https://numpy.org/doc/stable/reference/generated/numpy.transpose.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y-QUcWc3RXlk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [0.03496017 0.15017122 0.29256546]\n",
      "Final cost: 0.05080231395721322\n"
     ]
    }
   ],
   "source": [
    "# Example usage. Play around by adding more data points.\n",
    "X = np.array([[1, 1, 4], [1, 2, 6], [1, 3, 8], [1,7, 9], [1,24,5]]) # data with three columns: two features and one bias for each data point\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "theta_gd = np.zeros(3) # three parameters: two feature, and one bias.\n",
    "iterations = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "theta_gd, cost_history = gradient_descent(X, y, theta_gd, learning_rate, iterations)\n",
    "print(\"Theta:\", theta_gd)\n",
    "print(\"Final cost:\", cost_history[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrQpYpKOLhJd"
   },
   "source": [
    "## Tests for Gradient Descent and MSE Loss\n",
    "\n",
    "###<font color='red'>Students should **NOT** modify the test cell.</font>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_bKQDuiF-ie2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_compute_cost (__main__.TestOptimizationFunctions) ... ok\n",
      "test_gradient_descent (__main__.TestOptimizationFunctions) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.071s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestOptimizationFunctions(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Test data\n",
    "        self.X = np.array([[1, 2, 4, 5, 8], [1, 3, 6, 8, 9], [1, 4, 5, 4, 2], [1, 5, 1, 6, 8]])\n",
    "        self.y = np.array([7, 6, 5, 4])\n",
    "        self.t = np.array([0.1, 0.2, 0.1, 0.3, 0.8])\n",
    "        self.lr = 0.001\n",
    "        self.iter = 1000\n",
    "\n",
    "    def test_compute_cost(self):\n",
    "        # Compute the cost with known values\n",
    "        cc = compute_cost(self.X, self.y, self.t)\n",
    "        ec = 14.2625\n",
    "        self.assertAlmostEqual(cc, ec, places=4)\n",
    "\n",
    "\n",
    "    def test_gradient_descent(self):\n",
    "        # Run gradient descent to find optimized theta values\n",
    "        theta, c_history = gradient_descent(self.X, self.y, self.t, self.lr, self.iter)\n",
    "        t = np.round(theta,3)\n",
    "        e_t = np.array([0.241, 0.221, 0.688, -0.19, 0.4])\n",
    "        np.testing.assert_array_almost_equal(t, e_t, decimal=8)\n",
    "\n",
    "        # Check that the cost history is decreasing\n",
    "        for i in range(1, len(cost_history)):\n",
    "            self.assertLessEqual(c_history[i], c_history[i - 1])\n",
    "\n",
    "# Running the unit tests\n",
    "if __name__ == '__main__':\n",
    "    suite1 = unittest.TestLoader().loadTestsFromTestCase(TestOptimizationFunctions)\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    runner.run(suite1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAOlhOIJZ-B5"
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "Stochastic Gradient Descent (SGD) is a variant of the gradient descent algorithm. It uses only a single training example at a time to perform the updates. This approach speeds up the training process and used commonly.\n",
    "\n",
    "$\\text{For each training example } (x^{(i)}, y^{(i)}), \\text{ and for each parameter } \\theta, \\text{ update:} \\\\\n",
    "\\theta := \\theta - \\eta \\frac{\\partial}{\\partial \\theta} J(\\theta; x^{(i)}, y^{(i)})$\n",
    "\n",
    "\n",
    "Do not worry. You will not be coding SGD, however, it is important to know how SGD is implemented.\n",
    "\n",
    "MSE loss for a single training example is as follows:\n",
    "\n",
    "$    \n",
    "  J(\\theta; x^{(i)}, y^{(i)}) = MSE_i = (y_i - \\hat{y}_i)^2\n",
    "$\n",
    "\n",
    "The gradient of $MSE_i$ is computed as:\n",
    "\n",
    "$\n",
    "  \\nabla_{\\theta} MSE_i = -2 x_i (y_i - \\hat{y}_i)\n",
    "$\n",
    "\n",
    "Substituting the gradient in parameter update rule for SGD:\n",
    "\n",
    "$\n",
    "  \\theta_{new} = \\theta_{old} + 2\\eta x_i (y_i - \\hat{y}_i)\n",
    "$\n",
    "\n",
    "You can compare the above equation for SGD with update rule for gradient descent: $\\theta_{new} = \\theta_{old} - \\eta \\cdot \\frac{2}{n} \\sum_{i=1}^{n} x_{ij} (y_i - \\hat{y}_i)$. You can observe that there is no summation of $n$ training examples before the update. This way, SGD converges faster than GD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUouYrhLqEek"
   },
   "source": [
    "## Backpropagation Algorithm\n",
    "\n",
    "Backpropagation (backprop) is a fundamental algorithm that is used for training neural networks. It is an application of the chain rule from calculus to compute the gradient (or derivatives) of a loss function with respect to the weights of the network.\n",
    "\n",
    "\n",
    "You have learned about backpropagation algorithm in detail in the lecture. Here you will first implement backpropagation algorithm by hand and then implement it in pytorch.\n",
    "\n",
    "## Network Structure\n",
    "- **Inputs:** $(x_1, x_2)$\n",
    "- **Hidden Layer:** 2 neurons $(h_1, h_2$)\n",
    "- **Output:** Single neuron $(o_1)$\n",
    "- **Activation function:** Sigmoid for hidden and output layers\n",
    "- **Loss Function:** MSE Loss\n",
    "\n",
    "## Weights & Biases:\n",
    "- **Weights from input to hidden layer:** $(w_{11}, w_{12}, w_{21}, w_{22})$\n",
    "- **Biases of hidden layer:** $(b_1, b_2)$\n",
    "- **Weights from hidden to output layer:** $(w_{o1}, w_{o2})$\n",
    "- **Bias of output neuron:** $(b_o)$\n",
    "\n",
    "## Given Values:\n",
    "- **Input:** $(x_1 = 0.5, x_2 = 0.85)$\n",
    "- **True output (target):** $(y = 0.75)$\n",
    "- **Initial Weights:** $(w_{11} = 0.15, w_{12} = 0.25, w_{21} = 0.45, w_{22} = 0.35)$\n",
    "- **Initial Biases:** $(b_1 = 0.2, b_2 = 0.3)$\n",
    "- **Output Layer Weights:** $(w_{o1} = 0.6, w_{o2} = 0.4)$\n",
    "- **Output Layer Bias:** $(b_o = 0.1)$\n",
    "\n",
    "## Task:\n",
    "1. **Feedforward Phase:** Compute the output $(o_1)$ of the neural network.\n",
    "2. **Backpropagation Phase:** Update all weights and biases using the backpropagation algorithm assuming a learning rate $(\\eta = 0.1)$.\n",
    "\n",
    "The computations are given to help the students out.\n",
    "\n",
    "### Step 1: Feedforward Phase\n",
    "Calculate hidden neuron activations:\n",
    "- For each hidden neuron $(h_i)$:\n",
    "  - $z_i = w_{i1} \\cdot x_1 + w_{i2} \\cdot x_2 + b_i$\n",
    "  - $h_i = \\sigma(z_i)$ where $\\sigma(z)$ is the sigmoid activation function $\\sigma(z) = \\frac{1}{1 + e^{-z}}$.\n",
    "\n",
    "Calculate output neuron:\n",
    "- $z_o = w_{o1} \\cdot h_1 + w_{o2} \\cdot h_2 + b_o$\n",
    "- $o_1 = \\sigma(z_o)$\n",
    "\n",
    "### Step 2: Backpropagation Phase\n",
    "Compute output error:\n",
    "- Error (loss) derivative with respect to output:\n",
    "  - $\\frac{\\partial E}{\\partial o_1} = 2 \\cdot (o_1 - y)$\n",
    "- Derivative of the sigmoid function at the output:\n",
    "  - $\\frac{\\partial o_1}{\\partial z_o} = o_1 \\cdot (1 - o_1)$\n",
    "- Error (loss) with respect to $z_o$:\n",
    "  - $\\delta_o = \\frac{\\partial E}{\\partial o_1} \\cdot \\frac{\\partial o_1}{\\partial z_o}$\n",
    "\n",
    "Update weights and biases from hidden to output:\n",
    "- $w_{oi} \\leftarrow w_{oi} - \\eta \\cdot \\delta_o \\cdot h_i$\n",
    "- $b_o \\leftarrow b_o - \\eta \\cdot \\delta_o$\n",
    "\n",
    "\n",
    "Backpropagate error to hidden layer:\n",
    "- For each hidden neuron $i$:\n",
    "  - $\\delta_i = \\delta_o \\cdot w_{oi} \\cdot h_i \\cdot (1 - h_i)$\n",
    "\n",
    "Update weights and biases from input to hidden:\n",
    "- $w_{ij} \\leftarrow w_{ij} - \\eta \\cdot \\delta_i \\cdot x_j$\n",
    "- $b_i \\leftarrow b_i - \\eta \\cdot \\delta_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Answers:\n",
    "For each of the above computation, students need to perform only 1 iteration. After 1 iteration. Output the following values:\n",
    "\n",
    "1. \n",
    "$$\n",
    "\\begin{align*}\n",
    "h &= \\begin{bmatrix}\n",
    "0.6195 \\\\\n",
    "0.6948\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "2. $o_1$  = 0.6791\n",
    "3. $w_{o1}$  = 0.601\n",
    "4. $w_{o2}$ = 0.4011\n",
    "5. $b_o$ = 0.1015\n",
    "6. $\\delta_{h1}$ = -0.0093\n",
    "7. $\\delta_{h2}$ = -0.0062"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbbea36",
   "metadata": {},
   "source": [
    "# Building our own **reverse mode autodiff** \n",
    "\n",
    "In this exercise you will create your own rudimentary reverse mode automatic differentiation framework. For simplicity, our toy autodiff will work only with scalar parameters and result of each operation will also be a scalar. \n",
    "\n",
    "Our simple framework will only support the basic binary arithmetic operations -- addition, subtraction, multiplication, and division -- plus two unary operations exponentiation (e^x) and logarithm (log x).\n",
    "\n",
    "The skeleton of the framework operates on the notion of the **computation graph** which consist of **nodes**, implemented with the class **Node** below. The class consist of the following methods and properties:\n",
    "\n",
    "* Properties\n",
    " - *name* (string): name of the node (parameter or intermediate result of the computation graph)\n",
    " - *init_val* (float): initial value for the parameter (only for nodes that are input parameters)\n",
    " - *is_param* (bool): indicate if the node represents a trainable parameter (value is True) or an intermediate computation result (value is False)\n",
    " - *grad* (float): the value of the **adjoint** of the node (see the slides from Lecture 3). If the node is a parameter, then the value is the actual partial derivative of the objective function w.r.t. that parameter. \n",
    " - *parent1* (Node) and parent2 (Node): parent nodes of the node. Since we're supporting only unary and binary operations, then each node is the result of the operation on one or two other nodes. If unary operation, then self.parent2 is None. If the node is a trainable input parameter, then both parent nodes will be None (i.-e., input parameters have no parents in the computation graph) \n",
    " - *children* (list of Nodes): all nodes created with this node as a parent\n",
    " - *done*  (bool): indicates if the computation of the adjoint of the node is completed (i.e., contributions of all children nodes have been aggregated) \n",
    " \n",
    "* Methods\n",
    "  - constructor (*__init__*): initializes the new Node when created -- if the init_val is not provided and is_param is True (i.e., the Node is a trainable parameter), the value will be randomly initialized.\n",
    "  - *backward*: key function that propagates the computation of **adjoints** from the current node to the parent node. It is a recursive implementation of the reverse pass through the computation graph, starting from the objective (the Node on which backward() is called)\n",
    "  - *is_done*: checks if the computation of the adjoint of the node is completed, i.e., if all children *nodes* are labeled as done, and if so, marks this node as *done* too \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea662964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import typing\n",
    "import numpy as np\n",
    "\n",
    "# single node in the computation graph, either parameter or intermediate result\n",
    "class Node:\n",
    "    def __init__(self, name: str, init_val : float = None, is_param : bool = False):\n",
    "        self.name = name\n",
    "        if init_val:\n",
    "            self.value = init_val\n",
    "        elif is_param:\n",
    "            self.value = random.random()\n",
    "        \n",
    "        self.grad = None\n",
    "        self.done = False\n",
    "        \n",
    "        self.parent1 = None\n",
    "        self.parent2 = None\n",
    "        self.children = []\n",
    "        \n",
    "    def backward(self, target_func = False):\n",
    "        print(self.name)\n",
    "        if target_func:\n",
    "            self.grad = 1\n",
    "            self.done = True\n",
    "        \n",
    "        # initializing parent node's adjoint to 0 if current node is its first child processed (in reverse)\n",
    "        if self.parent1:\n",
    "            if not self.parent1.grad:\n",
    "                self.parent1.grad = 0\n",
    "            self.parent1.grad += self.grad * self.back_first\n",
    "\n",
    "        # false for unary operators\n",
    "        if self.parent2: \n",
    "            # initializing parent node's adjoint to 0 if current node is its first child processed (in reverse)\n",
    "            if not self.parent2.grad:\n",
    "                self.parent2.grad = 0\n",
    "            self.parent2.grad += self.grad * self.back_second\n",
    "        \n",
    "        if self.parent1 and self.parent1.is_done():\n",
    "            self.parent1.backward(target_func = False)\n",
    "        \n",
    "        if self.parent2 and self.parent2.is_done():\n",
    "            self.parent2.backward(target_func = False)\n",
    "        \n",
    "        \n",
    "    # indicates if the adjoint for the node can be computed, that is, \n",
    "    # if the adjoints of all children have been computed    \n",
    "    def is_done(self):\n",
    "        for c in self.children:\n",
    "            if not c.done:\n",
    "                return False\n",
    "        else:\n",
    "            self.done = True\n",
    "            return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb442bb6",
   "metadata": {},
   "source": [
    "**Task #1**: Complete the two incomplete lines of code above in the *backward* method of the **Node** class, those where the adjoint of the parents should be augmented with the contribution from the child (i.e., current node).  \n",
    "\n",
    "```python\n",
    "self.parent1.grad += ...\n",
    "\n",
    "...\n",
    "\n",
    "self.parent2.grad += ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d584f4a9",
   "metadata": {},
   "source": [
    "We next need to define the supported **operations** and in particular: \n",
    "- *forward* function: How the value of a child node is computed from the value of the parent node(s) for each particular operation\n",
    "- *back_first*: How the derivative of the child node variable per *first parent* node variable is computed\n",
    "- *back_second*: How the derivative of the child node variable per *second parent* node variable is computed. If the child node is computed from only one parent via an unary operation, than back_second is undefined. \n",
    "\n",
    "We first define the abstract class **Operation**, which will take care of all the \"code\" common to all operations (creation of the new node from parent(s)) except the exact computation of *forward*, *back_first* and *back_second*, which will have to be implemented in each class that inherits from Operation. \n",
    "\n",
    "In the constructor of the **Operation**, the new child node is created from one or two input nodes (parent nodes). The actual \"semantics\" of the Operation, that is, (1) how the value of the child node is to be computed from the value(s) of the parent node(s) -- function *forward()* and (2) how the adjoint of the child node is to contribute to the adjoint of the parent node(s) (i.e., the derivative of the child per parent) are to be implemented in classes that inherit from the class Operation and represent concrete operations: \n",
    "\n",
    "- Add: value of the child node is the sum of values of parent nodes\n",
    "- Sub: value of the child node is the value of the second parent subtracted from the value of the first parent\n",
    "- Mul: value of the child node is the product of values of parent nodes\n",
    "- Div: value of the child node is the values of the first parent node divided with the value of the second parent node\n",
    "- Exp: value of the child is e to the power of the value of the parent node\n",
    "- Log: value of the child is the natural logarithm of the value of the parent node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7162f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation:\n",
    "    def __init__(self, input1: Node, input2: Node = None, name: str = None):\n",
    "        self.input1 = input1\n",
    "        self.input2 = input2\n",
    "        \n",
    "        self.output = Node(name = name, is_param = False)\n",
    "        \n",
    "        self.output.value = self.forward()\n",
    "        print(self.output.name + \" \" + str(self.output.value))\n",
    "        \n",
    "        self.input1.children.append(self.output)\n",
    "        self.output.parent1 = input1\n",
    "        \n",
    "        self.output.back_first = self.back_first()\n",
    "        \n",
    "        if self.input2:\n",
    "            self.input2.children.append(self.output)\n",
    "            self.output.parent2 = input2\n",
    "            \n",
    "            self.output.back_second = self.back_second()\n",
    "    \n",
    "    # specifies how the output of the operation is computed from the inputs\n",
    "    def forward(self):  \n",
    "        raise NotImplementedError(\"Abstract function. Implement in the child class\")\n",
    "    \n",
    "    # specifies how the partial derivative of output per first input is computed\n",
    "    def back_first(self):\n",
    "        raise NotImplementedError(\"Abstract function. Implement in the child class\")    \n",
    "    \n",
    "    def back_second(self):\n",
    "        raise NotImplementedError(\"Abstract function. Implement in the child class\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e14072",
   "metadata": {},
   "source": [
    "**Task #2**: Implement the **forward**, **back_first**, and (for binary operators) **back_second** function for the six listed basic operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4376a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class Add(Operation):\n",
    "    def __init__(self, input1: Node, input2: Node = None, name: str = None):\n",
    "        super(Add, self).__init__(input1, input2, name)\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.input1.value + self.input2.value\n",
    "    \n",
    "    def back_first(self):\n",
    "        return 1\n",
    "    \n",
    "    def back_second(self):\n",
    "        return 1\n",
    "\n",
    "class Sub(Operation):\n",
    "    def __init__(self, input1: Node, input2: Node = None, name: str = None):\n",
    "        super(Sub, self).__init__(input1, input2, name)\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.input1.value - self.input2.value\n",
    "    \n",
    "    def back_first(self):\n",
    "        return 1\n",
    "    \n",
    "    def back_second(self):\n",
    "        return -1    \n",
    "\n",
    "class Div(Operation):\n",
    "    def __init__(self, input1: Node, input2: Node = None, name: str = None):\n",
    "        super(Div, self).__init__(input1, input2, name)\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.input1.value / self.input2.value\n",
    "    \n",
    "    def back_first(self):\n",
    "        return 1 / self.input2.value\n",
    "    \n",
    "    def back_second(self):\n",
    "        return -self.input1.value / (self.input2.value ** 2)\n",
    "    \n",
    "    \n",
    "class Mul(Operation):\n",
    "    def __init__(self, input1: Node, input2: Node = None, name: str = None):\n",
    "        super(Mul, self).__init__(input1, input2, name)\n",
    "    \n",
    "    def forward(self):\n",
    "        return self.input1.value * self.input2.value\n",
    "    \n",
    "    def back_first(self):\n",
    "        return self.input2.value\n",
    "    \n",
    "    def back_second(self):\n",
    "        return self.input1.value\n",
    "\n",
    "class Exp(Operation):\n",
    "    def __init__(self, input1: Node, input2: Node = None, name: str = None):\n",
    "        super(Exp, self).__init__(input1, input2, name)\n",
    "    \n",
    "    def forward(self):\n",
    "        return math.exp(self.input1.value)\n",
    "    \n",
    "    def back_first(self):\n",
    "        return math.exp(self.input1.value)\n",
    "        \n",
    "class Log(Operation):\n",
    "    def __init__(self, input1: Node, input2: Node = None, name: str = None):\n",
    "        super(Log, self).__init__(input1, input2, name)\n",
    "    \n",
    "    def forward(self):\n",
    "        return math.log(self.input1.value)\n",
    "    \n",
    "    def back_first(self):\n",
    "        return math.log(self.input1.value)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be85615",
   "metadata": {},
   "source": [
    "**Task #3**: Your next task is now to use the above operations to create a computation graph for the following function: \n",
    "\n",
    "$$ f(x_1, x_2) = (\\frac{x_1}{x_2} + e^{\\frac{x_1}{x_2}})\\cdot(\\frac{x_1}{x_2} - \\mathit{ln}\\,{x_2}) $$\n",
    "\n",
    "Yes, it's the same function from the lecture slides, so that you can easily check the results (values and adjoints) :)). Note: the result (i.e, child node) of the operation is obtainable through the *.output* property of the corresponding Operation object. \n",
    "\n",
    "The computation graph you should create is the following: \n",
    "\n",
    "- $$ a = \\frac{x_1}{x_2} $$\n",
    "- $$ b = e^{a} $$\n",
    "- $$ c = a + b $$\n",
    "- $$ d = \\mathit{ln}\\,x_2 $$\n",
    "- $$ e = a - d $$\n",
    "- $$ f = c \\cdot e $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70cf20db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.5\n",
      "b 1.6487212707001282\n",
      "c 2.148721270700128\n",
      "d 0.7648729073870885\n",
      "e -0.26487290738708846\n",
      "f -0.5691380501348221\n"
     ]
    }
   ],
   "source": [
    "x1 = Node(name=\"x1\", init_val = 0.5, is_param = True)\n",
    "x2 = Node(name=\"x2\", init_val = 1.0, is_param = True)\n",
    "\n",
    "a = Div(x1, x2, \"a\").output\n",
    "b = Exp(a, name = \"b\").output\n",
    "c = Add(b, x1, \"c\").output\n",
    "d = Log(c, name = \"d\").output\n",
    "e = Sub(a, d, \"e\").output\n",
    "f = Mul(c, e, name=\"f\").output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72452442",
   "metadata": {},
   "source": [
    "Finally, we will call the reverse mode pass of the autodiff, by invoking the backward function on the final node of our computation graph, stored in the variable *f* (representing the whole function *f*). The recursive backward function will print out the names of the nodes in the order in which they are visited in the reverse/backward pass through the computation graph.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "748240df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f\n",
      "e\n",
      "d\n",
      "c\n",
      "b\n",
      "a\n",
      "x1\n",
      "x2\n",
      "x1\n"
     ]
    }
   ],
   "source": [
    "f.backward(target_func = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f60bc",
   "metadata": {},
   "source": [
    "After running the backward/reverse pass, we can check the values of the gradient of f per our input variables $$x_1$$ and $$x_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74d85138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.9060231597397554\n",
      "0.4988257834338905\n"
     ]
    }
   ],
   "source": [
    "print(x1.grad)\n",
    "print(x2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luYtBUMWWv4c"
   },
   "source": [
    "## Implementation in Pytorch\n",
    "\n",
    "Now you will implement backpropgation algorithm using [pytorch autograd module](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) on a neural network which we will construct using [pytorch.nn](https://pytorch.org/docs/stable/nn.html).\n",
    "\n",
    "Kindly read the documentation for a detailed overview. Helpful functions for the exercise are provided below.\n",
    "\n",
    "Important functions in the pytorch library:\n",
    "\n",
    "1. **torch.nn.Module**: The base class for all neural network modules. Your custom models should also subclass this class. Models are defined by subclassing nn.Module and defining a forward method.\n",
    "\n",
    "2. **torch.nn.Linear(input_size, hidden_size)**: It's used to define the layers of the neural network. The input_size and hidden_size parameters specify the size of each input and output, respectively\n",
    "\n",
    "3. **torch.nn.ReLU()**: Applies the rectified linear unit function element-wise. It's used as the activation function between layers to introduce non-linearity.\n",
    "\n",
    "4. torch.nn.Sigmoid(): Applies the sigmoid function element-wise. It's used here in the output layer to squash the output to a range between 0 and 1, suitable for binary classification.\n",
    "\n",
    "5. **torch.nn.BCELoss()**: Commonly used binary cross entropy objective function. It's used as the loss function for training a classifier.\n",
    "\n",
    "6. **torch.optim.SGD(model.parameters(), lr=learning_rate)**:  Implements stochastic gradient descent (SGD) optimization. It updates the model's weights using the gradients computed during backpropagation. The lr parameter specifies the learning rate.\n",
    "\n",
    "Important function in the pytorch autograd:\n",
    "\n",
    "1. **with torch.no_grad()**: A context manager that disables gradient calculation, making code run faster and reducing memory usage. It's used here during the evaluation phase where we don't need gradients.\n",
    "\n",
    "2. **loss.backward()**: Computes the gradient of the loss with respect to all tensors. After calling this function, all tensors in the computation graph will have their **.grad** tensor populated with the gradients.\n",
    "\n",
    "3. **optimizer.zero_grad()**: Clears the gradients of all optimized tensors. This is necessary because by default, gradients accumulate in buffers whenever .backward() is called.\n",
    "\n",
    "4. **optimizer.step()**: Performs a single optimization step (parameter update)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ba1rxqcdozVh"
   },
   "outputs": [],
   "source": [
    "#import the necessary modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_pt1E8Gv05Ft"
   },
   "source": [
    "## Implement the following functions:\n",
    "1. **forward()**\n",
    "2. **train()**\n",
    "3. **Variables in main**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Oyrp9QNtthsW"
   },
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyNeuralNetwork, self).__init__()\n",
    "        # Implement layers\n",
    "        # Neural Network Architecture:\n",
    "        # Input layer --> Linear --> ReLU --> Linear --> Sigmoid --> Output layer\n",
    "        #\n",
    "        # To add layers:\n",
    "        # Simply add self.fcX = nn.Linear(previous_layer_size, new_layer_size) lines\n",
    "        # and apply activations between them as needed in the forward function.\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        # Feed the input x in the first fully connected layer, and then into non linear layer, and so on.\n",
    "        #At the end, use sigmoid activation function.\n",
    "        # x = self.fc1(x)\n",
    "        # x = self.relu(x)\n",
    "\n",
    "        return self.sigmoid(self.fc2(self.relu(self.fc1(x))))\n",
    "\n",
    "def train(model, criterion, optimizer, X_train, y_train, epochs=100):\n",
    "    loss_history = []\n",
    "\n",
    "    model.train()\n",
    "    # for each training epoch\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y = model(X_train).squeeze()\n",
    "        # Compute and save loss history\n",
    "        loss = criterion(y, y_train)\n",
    "        loss_history.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Zero gradients\n",
    "\n",
    "\n",
    "        #perform a backward pass.\n",
    "\n",
    "\n",
    "        #update the weights\n",
    "\n",
    "\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # return loss history\n",
    "    return loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will also compute **accuracy metric**.\n",
    "\n",
    "Accuracy = $\\frac{Number\\;of\\;correct\\;predictions}{Number\\;of\\;total\\;predictions}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, X, y):\n",
    "  # Compute Accuracy Metric.\n",
    "  # Use torch.no_grad()\n",
    "  # Get model predictions\n",
    "  # Get total number of predictions\n",
    "  # Get the total number of correct predictions\n",
    "  # compute accuracy. multiply by 100 to get percentage.\n",
    "  with torch.no_grad():\n",
    "      model.eval()\n",
    "      predictions = model(X).squeeze()\n",
    "      total = y.shape[0]\n",
    "      correct = (predictions > 0.5) == y\n",
    "      accuracy = correct.sum().item() / total * 100\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "QyjnpYA6vKxt",
    "outputId": "32110375-2728-4316-9989-959614184fc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.6791\n",
      "Epoch [20/100], Loss: 0.6751\n",
      "Epoch [30/100], Loss: 0.6712\n",
      "Epoch [40/100], Loss: 0.6672\n",
      "Epoch [50/100], Loss: 0.6633\n",
      "Epoch [60/100], Loss: 0.6594\n",
      "Epoch [70/100], Loss: 0.6555\n",
      "Epoch [80/100], Loss: 0.6516\n",
      "Epoch [90/100], Loss: 0.6477\n",
      "Epoch [100/100], Loss: 0.6438\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.0%\n"
     ]
    }
   ],
   "source": [
    "# Dummy dataset generation\n",
    "def generate_dummy_data(num_samples=100, input_size=10):\n",
    "    X = torch.randn(num_samples, input_size)\n",
    "    y = (torch.sum(X, dim=1) > 0).float()  # Simple classification task based on the sum of features\n",
    "    return X, y\n",
    "\n",
    "# Main function\n",
    "\n",
    "# TODO: declare model, criterion, and optimizer variables.\n",
    "if __name__ == '__main__':\n",
    "    input_size = 10\n",
    "    hidden_size = 8\n",
    "    output_size = 1\n",
    "    learning_rate = 0.01\n",
    "    epochs = 100\n",
    "\n",
    "    X, y = generate_dummy_data()\n",
    "\n",
    "    #Create your model by calling the class MyNeuralNetwork\n",
    "    model = MyNeuralNetwork(input_size, hidden_size, output_size)\n",
    "    # Use binary cross entropy as your cost function\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    # Use stochastic gradient descent as your optimizer.\n",
    "    # Make sure to pass model.parameter() and the learning rate to optimizer.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    loss_history = train(model, criterion, optimizer, X, y, epochs)\n",
    "\n",
    "    # Plotting loss history\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss history')\n",
    "    plt.show()\n",
    "\n",
    "    # Compute and print accuracy\n",
    "    accuracy = compute_accuracy(model, X, y)\n",
    "    print(f'Accuracy: {accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5wMsMry5SrVM"
   },
   "source": [
    "## Tests for Backprop\n",
    "\n",
    "###<font color='red'>Students should **NOT** modify the test cell.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "_yCausoUSVoT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_accuracy_computation (__main__.TestNeuralNetwork) ... ok\n",
      "test_forward (__main__.TestNeuralNetwork) ... ok\n",
      "test_initialization (__main__.TestNeuralNetwork) ... ok\n",
      "test_train_function (__main__.TestNeuralNetwork) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.012s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.6587\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestNeuralNetwork(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.input_size = 10\n",
    "        self.hidden_size = 15\n",
    "        self.output_size = 1\n",
    "        self.model = MyNeuralNetwork(self.input_size, self.hidden_size, self.output_size)\n",
    "\n",
    "    def test_initialization(self):\n",
    "        # Check if the model is correctly initialized with the expected layers\n",
    "        self.assertIsInstance(self.model.fc1, nn.Linear)\n",
    "        self.assertIsInstance(self.model.fc2, nn.Linear)\n",
    "        self.assertEqual(self.model.fc1.in_features, self.input_size)\n",
    "        self.assertEqual(self.model.fc1.out_features, self.hidden_size)\n",
    "        self.assertEqual(self.model.fc2.in_features, self.hidden_size)\n",
    "        self.assertEqual(self.model.fc2.out_features, self.output_size)\n",
    "\n",
    "    def test_forward(self):\n",
    "        # Check the forward pass\n",
    "        x = torch.randn(5, self.input_size)\n",
    "        output = self.model(x)\n",
    "        self.assertEqual(output.shape, (5, self.output_size))\n",
    "        # Ensure output is in the range [0, 1]\n",
    "        self.assertTrue(torch.all(output >= 0) and torch.all(output <= 1))\n",
    "\n",
    "    def test_train_function(self):\n",
    "        # Set up for training test\n",
    "        c = nn.BCELoss()\n",
    "        opt = optim.SGD(self.model.parameters(), lr=0.01)\n",
    "        X_train, y_train = generate_dummy_data(num_samples=100, input_size=self.input_size)\n",
    "\n",
    "        # Test the train function\n",
    "        loss_history = train(self.model, c, opt, X_train, y_train, epochs=10)\n",
    "        self.assertIsInstance(loss_history, list)\n",
    "        self.assertEqual(len(loss_history), 10)\n",
    "        # Ensure the loss is decreasing\n",
    "        self.assertTrue(all(loss_history[i] >= loss_history[i + 1] for i in range(len(loss_history) - 1)))\n",
    "\n",
    "    def test_accuracy_computation(self):\n",
    "        # Generate test data\n",
    "        X_test, y_test = generate_dummy_data(num_samples=100, input_size=self.input_size)\n",
    "\n",
    "        # Ensure the model has some predictions\n",
    "        self.model.eval()  # set the model to evaluation mode\n",
    "        accuracy = compute_accuracy(self.model, X_test, y_test)\n",
    "        self.assertIsInstance(accuracy, float)\n",
    "        self.assertTrue(0 <= accuracy <= 100)\n",
    "\n",
    "# Running the unit tests\n",
    "if __name__ == '__main__':\n",
    "    suite2 = unittest.TestLoader().loadTestsFromTestCase(TestNeuralNetwork)\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    runner.run(suite2)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "IRtoR8LgSx0W",
    "RrQpYpKOLhJd"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
