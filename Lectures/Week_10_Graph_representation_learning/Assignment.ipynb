{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 09\n",
    "\n",
    "In this exercise, we will further explore graph representation learning approaches and, in particular, Node2Vec. If you have questions regarding the exercise please use the WueCampus forum or contact Moritz Lampert via email (moritz.lampert@uni-wuerzburg.de). The exercise is due on July 9th 14:00 (UTC+2)\n",
    "\n",
    "## Setup\n",
    "\n",
    "The exercise uses the Python Library `PathPyG` that is based on the deep learning library [PyTorch](https://pytorch.org/docs/stable/index.html) and the graph learning library [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/). `PathPyG` enables interactive graph visualisations and implements a variety of algorithms for graphs and temporal graphs. For further information and installation instructions, see [pathpy.net](https://www.pathpy.net/0.1.0-dev/getting_started/) or the Jupyter Notebooks of Week 1 of the [GitLab repository](https://gitlab.informatik.uni-wuerzburg.de/ml4nets_notebooks/2024_sose_ml4nets_notebooks) that accompanies our Lecture Machine Learning for Complex Networks (ML4Nets) that was already used in the practice sessions of the lecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from torch import tensor\n",
    "from torch.nn import Module\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from pathpyG import Graph, plot, IndexMap\n",
    "from pathpyG.processes import RandomWalk\n",
    "from pathpyG.io import read_netzschleuder_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to `PathPyG`\n",
    "\n",
    "In the following, you can find a brief introduction into PyTorch Geometric (PyG) and PathPyG. For more information refer to the tutorials in the corresponding documentations or the aforementioned notebooks of our ML4Nets Course.\n",
    "\n",
    "### 1. Graphs in `PyG`\n",
    "\n",
    "Mathematically, the edges of a graph are commonly represented as adjacency matrix $A \\in \\{0,1\\}^{n\\times n}$ where $n$ is the number of nodes. An element $A_{ij}$ is 1 iff the nodes $v_i$ and $v_j$ are connected via an edge. For large-scale graphs, this representation is not very space-efficient since real-world graphs are typically very sparse (most entries are 0). Thus, the default edge representation that is used in `PyG` is not an adjacency matrix but a so-called `edge_index` instead. An `edge_index` is a `Tensor` with shape `(2, m)` where $m$ is the number of edges in the graph and contains for each edge $e_i=(v_j, v_k)$ the indices `edge_index[0, i] = j` and `edge_index[1, i] = k`.\n",
    "\n",
    "Let's create an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = tensor([\n",
    "    [0, 0, 1, 1, 3, 3, 3, 4, 5],\n",
    "    [1, 2, 3, 2, 5, 4, 6, 5, 6]\n",
    "])\n",
    "print(f\"Source Node of edge 2: {edge_index[0, 1]}\")\n",
    "print(f\"Destination Node of edge 2: {edge_index[1, 1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most basic graph representation used in `PyG`. To add more properties to a graph, e.g. node or edge weights, we need to wrap the `edge_index` with a `Data` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Data(edge_index=edge_index)\n",
    "\n",
    "g[\"node_weight\"] = tensor([1, 1, 1, 0, 0, 0, 0])\n",
    "g[\"edge_weight\"] = tensor([1, 2, 1, 1, 1, 2, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Graph Visualisations in PathPyG\n",
    "\n",
    "For visualisation, we need to create a `pathpyG.Graph`-object. We can visualise the node weights - that we can access using the `data`-variable - for example as different colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Graph(g, mapping=IndexMap([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"]))\n",
    "\n",
    "colors = array([\"red\", \"blue\"])\n",
    "plot(net, node_color=colors[net.data[\"node_weight\"]].tolist(), node_label=list(net.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the above graph is directed. To create an undirected graph, you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to_undirected()\n",
    "\n",
    "colors = array([\"red\", \"blue\"])\n",
    "plot(net, node_color=colors[net.data[\"node_weight\"]].tolist(), node_label=list(net.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Walks\n",
    "\n",
    "As you learned in the lecture, we need to sample random walks on the graph so that we can apply node2vec. We can use `PathPyG` for this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_walks(net, num_walks, walk_length):\n",
    "    rw = RandomWalk(net)\n",
    "    data = rw.run_experiment(runs=num_walks, steps=walk_length)\n",
    "    walk_data = rw.get_paths(data)\n",
    "    walks = [walk_data.get_walk(i) for i in range(walk_data.num_paths)]\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = sample_random_walks(net, num_walks=2, walk_length=6)\n",
    "walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for simplicity, we will only implement random walks with $p=1$ and $q=1$, thus, essentially only DeepWalk and not Node2Vec. The Jupyter Notebook 3 of Week 10 from the ML4Nets course that is also referenced as practice session in the lecture explores how to implement random walk sampling with different values for $p$ and $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "### 1. Continuous Bag-of-Words Model\n",
    "\n",
    "The original paper that introduces Word2Vec ([T. Mikolov et al.](https://arxiv.org/pdf/1301.3781)) proposes multiple models to optimize the embedding. You learned about one option - the so-called Skip-Gram Model - in the lecture. In this task, we explore the alternative: The Continuous Bag-of-Words Model (CBOW)\n",
    "\n",
    "#### a) What is CBOW?\n",
    "\n",
    "Check out the referenced paper or other resources to learn about CBOW. Explain how it works and what the differences to Skip-Gram are with your own words. What does this mean in the graph context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Insert CBOW explanation and differences to Skip-Gram here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Implement CBOW\n",
    "\n",
    "##### Utility Functions\n",
    "For the implementation, we can reuse some parts of the implementation from the notebooks presented in the lecture. Other methods need to be implemented differently. Complete the methods below so that they will work for CBOW. You can copy the code from the notebooks if suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens):\n",
    "    # TODO\n",
    "    return NotImplementedError()\n",
    "\n",
    "def build_word_context_tuples(tokens, window_size):\n",
    "    # TODO\n",
    "    return NotImplementedError()\n",
    "\n",
    "def get_ohes(words, vocab):\n",
    "    # TODO\n",
    "    return NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Checks\n",
    "print(\"Walks:\")\n",
    "print(walks, \"\\n\")\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "vocab = build_vocabulary(walks)\n",
    "print(vocab, \"\\n\")\n",
    "\n",
    "print(\"Word + Context:\")\n",
    "word_context_tuples = build_word_context_tuples(walks, window_size=2)\n",
    "print(word_context_tuples, \"\\n\")\n",
    "\n",
    "print(\"One-Hot Encodings:\")\n",
    "print(get_ohes(word_context_tuples[0][0], vocab))\n",
    "print(get_ohes(word_context_tuples[0][1], vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBOW Model\n",
    "\n",
    "Define a `Torch`-`Module` that implements the CBOW model below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, vocab_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        return NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "cbow = CBOW(3, len(vocab))\n",
    "probs = cbow(get_ohes(word_context_tuples[0][1], vocab))\n",
    "assert len(vocab) == probs.size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop\n",
    "\n",
    "Define a training loop to train your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = sample_random_walks(net, num_walks=100, walk_length=10)\n",
    "vocab = build_vocabulary(walks)\n",
    "word_context_tuples = build_word_context_tuples(walks, window_size=2)\n",
    "model = CBOW(embedding_dim=5, vocab_size=len(vocab))\n",
    "\n",
    "# TODO: Initialize loss function and optimizer\n",
    "\n",
    "epochs = 50\n",
    "losses = []\n",
    "for i in range(epochs):\n",
    "    l = 0\n",
    "    # TODO: Inner Training Loop\n",
    "\n",
    "    losses.append(l/len(word_context_tuples))\n",
    "plt.plot(range(epochs), losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "def get_embedding(node, model, vocab):\n",
    "    return model.embeddings.weight.data[:,vocab.index(node)]\n",
    "\n",
    "svd = TruncatedSVD()\n",
    "low_dim = svd.fit_transform(array([get_embedding(w, model, vocab).detach().numpy() for w in vocab]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(low_dim[:,0], low_dim[:,1])\n",
    "\n",
    "for i, txt in enumerate(vocab):\n",
    "    plt.arrow(0, 0, low_dim[i,0], low_dim[i,1], color='orange', width=0.02, alpha=0.2)\n",
    "    ax.annotate(txt, (low_dim[i,0]+0.02, low_dim[i,1]+0.02), fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Link Prediction\n",
    "\n",
    "As explained in the lecture, the node embeddings created above can be used for a variety of tasks like node classification or link prediction.\n",
    "\n",
    "#### Hadamard-based Link Prediction using Node Embeddings\n",
    "\n",
    "The lecture briefly mentioned that we can use, e.g. the Hadamard product $x_i \\cdot x_j$, as similarity score to predict the existence of edges given embedding vectors $x_i$ and $x_j$ for nodes $v_i$ and $v_j$. Complete the following code snippet that predicts if an edge exists between two nodes given their label using the Hadamard product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_edge(node_i, node_j, model, vocab):\n",
    "    # TODO\n",
    "    return NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "print(\"High expected score: \", predict_edge(\"a\", \"c\", model, vocab))\n",
    "print(\"Low expected score: \", predict_edge(\"a\", \"f\", model, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real-World Example\n",
    "\n",
    "Evaluate the performance of your link predictor on the Karate Club network that you know from the lecture using the area under the curve of the receiver-operator characteristic. Since the network is comparably small, you can either compute the score for all possible node pairs or use a sample of non-existing links (negative sampling).\n",
    "\n",
    "*Note: The Karate Club network is a particularly difficult example network for link prediction. Thus, the results will most likely be not very good. If you want, you can also choose a different example network.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "karate = read_netzschleuder_network(\"karate\", \"77\")\n",
    "karate.mapping = IndexMap([str(i) for i in karate.data.node_name])\n",
    "\n",
    "plot(karate, node_label=list(karate.nodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
